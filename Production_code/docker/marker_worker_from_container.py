"Marker Worker Service\n\nConsumes DOCUMENT_UPLOADED events and runs Marker extraction in a GPU-accelerated environment.\n\nFeatures:\n1. Event-driven processing via message broker\n2. GPU pool management for high-volume processing\n3. Automatic retry with exponential backoff\n4. Graceful shutdown handling\n5. Metrics and observability\n"\nimport os\nfrom pathlib import Path\nimport asyncio\nimport json\nimport subprocess\nimport tempfile\nfrom datetime import datetime\nfrom typing import Optional\nimport signal\nimport aio_pika\n\nfrom opentelemetry import trace\n\n# Import common services from the shared module\nfrom services.common_services import (\n    StorageService,\n    MessageBrokerService,\n    DatabaseService,\n    DocumentStatus,\n)\n\n\n# =========================================================\n# Configuration\n# =========================================================\n\n\nclass MarkerWorkerConfig:\n    """Configuration for Marker worker"""\n\n    def __init__(self):\n        # Marker settings\n        self.MARKER_BATCH_SIZE = int(os.getenv("MARKER_BATCH_SIZE", "10"))\n        self.MARKER_USE_GPU = os.getenv("MARKER_USE_GPU", "true").lower() == "true"\n        self.MARKER_MAX_PAGES = int(os.getenv("MARKER_MAX_PAGES", "0")) or None\n\n        # Worker settings\n        self.CONCURRENT_WORKERS = int(os.getenv("CONCURRENT_WORKERS", "2"))\n        self.MAX_RETRIES = int(os.getenv("MAX_RETRIES", "3"))\n        self.RETRY_BACKOFF_BASE = int(os.getenv("RETRY_BACKOFF_BASE", "2"))\n\n        # Message broker\n        self.QUEUE_NAME = os.getenv("MARKER_QUEUE_NAME", "marker_extraction_queue")\n        self.EXCHANGE_NAME = os.getenv("DOCUMENT_EXCHANGE", "document_processing")\n        \n        # Paths\n        self.TEMP_DIR = Path(os.getenv("TEMP_DIR", "/tmp/marker_worker"))\n        self.OUTPUT_DIR = Path(os.getenv("OUTPUT_DIR", "/app/marker_output"))\n        self.TEMP_DIR.mkdir(parents=True, exist_ok=True)\n        self.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\n        # Database and Broker URLs from environment variables\n        self.POSTGRES_USER = os.getenv("POSTGRES_USER", "emc_user")\n        self.POSTGRES_PASSWORD = os.getenv("POSTGRES_PASSWORD", "password")\n        self.POSTGRES_DB = os.getenv("POSTGRES_DB", "emc_registry")\n        self.POSTGRES_HOST = os.getenv("POSTGRES_HOST", "postgres")\n        self.DATABASE_URL = f"postgresql://{self.POSTGRES_USER}:{self.POSTGRES_PASSWORD}@{self.POSTGRES_HOST}/{self.POSTGRES_DB}"\n\n        self.RABBITMQ_USER = os.getenv("RABBITMQ_USER", "emc")\n        self.RABBITMQ_PASSWORD = os.getenv("RABBITMQ_PASSWORD", "changeme")\n        self.RABBITMQ_HOST = os.getenv("RABBITMQ_HOST", "rabbitmq")\n        self.RABBITMQ_URL = f"amqp://{self.RABBITMQ_USER}:{self.RABBITMQ_PASSWORD}@{self.RABBITMQ_HOST}/"\n        \n        self.S3_BUCKET = os.getenv("S3_BUCKET", "emc-documents")\n\n\nconfig = MarkerWorkerConfig()\ntracer = trace.get_tracer(__name__)\n\n\n# =========================================================\n# Marker Execution\n# =========================================================\n\n\nclass MarkerExecutor:\n    """Handles Marker CLI execution"""\n\n    def __init__(self, use_gpu: bool = True):\n        self.use_gpu = use_gpu\n\n    @tracer.start_as_current_span("run_marker_extraction")\n    async def extract_document(\n        self,\n        input_pdf_path: Path,\n        output_dir: Path,\n        doc_id: str,\n    ) -> dict:\n        """\n        Run Marker extraction on a PDF.\n\n        Returns:\n            dict with extraction results and metadata\n        """\n        span = trace.get_current_span()\n        span.set_attribute("document.id", doc_id)\n        span.set_attribute("marker.use_gpu", self.use_gpu)\n\n        start_time = datetime.utcnow()\n\n        # Prepare output directory\n        doc_output_dir = output_dir / doc_id\n        doc_output_dir.mkdir(parents=True, exist_ok=True)\n\n        # Build Marker command\n        cmd = [\n            "marker_single",\n            str(input_pdf_path),\n            str(doc_output_dir),\n            "--batch_multiplier",\n            str(config.MARKER_BATCH_SIZE),\n        ]\n\n        if not self.use_gpu:\n            cmd.extend(["--disable_gpu"])\n\n        if config.MARKER_MAX_PAGES:\n            cmd.extend(["--max_pages", str(config.MARKER_MAX_PAGES)])\n\n        try:\n            # Run Marker as subprocess\n            process = await asyncio.create_subprocess_exec(\n                *cmd,\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE,\n            )\n\n            stdout, stderr = await process.communicate()\n\n            if process.returncode != 0:\n                error_msg = stderr.decode("utf-8")\n                span.set_status(trace.Status(trace.StatusCode.ERROR, error_msg))\n                raise RuntimeError(f"Marker failed: {error_msg}")\n\n            # Find generated JSON file\n            json_file = doc_output_dir / f"{input_pdf_path.stem}.json"
            if not json_file.exists():\n                # Marker might use original filename, let's find the first json\n                json_files = list(doc_output_dir.glob("*.json"))\n                if json_files:\n                    json_file = json_files[0]\n                else:\n                    raise FileNotFoundError("Marker did not generate JSON output")\n\n            # Load and validate JSON\n            with open(json_file, "r") as f:\n                json_data = json.load(f)\n\n            # Count pages\n            page_count = self._count_pages(json_data)\n\n            end_time = datetime.utcnow()\n            processing_time = (end_time - start_time).total_seconds()\n\n            span.set_attribute("marker.page_count", page_count)\n            span.set_attribute("marker.processing_time", processing_time)\n\n            return {\n                "success": True,\n                "json_path": str(json_file),\n                "output_dir": str(doc_output_dir),\n                "page_count": page_count,\n                "processing_time_seconds": processing_time,\n                "marker_version": self._get_marker_version(),\n            }\n\n        except Exception as e:\n            span.record_exception(e)\n            span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))\n            return {"success": False, "error": str(e)}\n\n    def _count_pages(self, json_data: dict) -> int:\n        """Count pages in Marker JSON output"""\n        # This is a simplified page count. For a more robust count,\n        # you might need to inspect the structure of your specific marker output.\n        return len(json_data.get("pages", []))\n\n    def _get_marker_version(self) -> str:\n        """Get Marker version"""\n        try:\n            result = subprocess.run(\n                ["marker_single", "--version"],\n                capture_output=True,\n                text=True,\n                timeout=5,\n            )\n            return result.stdout.strip()\n        except:\n            return "unknown"\n\n\n# =========================================================\n# Event Consumer\n# =========================================================\n\n\nclass MarkerWorker:\n    """\n    Worker that consumes DOCUMENT_UPLOADED events and runs Marker extraction.\n    """\n\n    def __init__(\n        self,\n        storage_service: StorageService,\n        database_service: DatabaseService,\n        message_broker_service: MessageBrokerService,\n        executor: MarkerExecutor,\n    ):\n        self.storage = storage_service\n        self.database = database_service\n        self.message_broker = message_broker_service\n        self.executor = executor\n\n        self.running = False\n        self.processing_tasks = set()\n\n    async def start(self):\n        """Start consuming events"""\n        self.running = True\n        print(f"[MARKER WORKER] Starting worker...")\n        print(f"[MARKER WORKER] GPU enabled: {config.MARKER_USE_GPU}")\n        print(f"[MARKER WORKER] Concurrent workers: {config.CONCURRENT_WORKERS}")\n\n        # Setup signal handlers for graceful shutdown\n        loop = asyncio.get_running_loop()\n        for sig in (signal.SIGTERM, signal.SIGINT):\n            loop.add_signal_handler(sig, self._signal_handler)\n\n        # Start consuming events\n        await self._consume_events()\n\n    def _signal_handler(self, signum, frame=None):\n        """Handle shutdown signals"""\n        print(\n            f"\n[MARKER WORKER] Received signal {signal.strsignal(signum)}, shutting down gracefully..."\n        )\n        self.running = False\n\n    async def _consume_events(self):\n        """\n        Consume events from message broker using aio_pika.\n        """\n        queue_name = config.QUEUE_NAME\n        exchange_name = config.EXCHANGE_NAME\n        \n        connection = None\n        \n        while self.running:\n            try:\n                # Connect to RabbitMQ\n                connection = await aio_pika.connect_robust(config.RABBITMQ_URL)\n                \n                async with connection:\n                    channel = await connection.channel()\n\n                    # Declare exchange\n                    exchange = await channel.declare_exchange(\n                        exchange_name, aio_pika.ExchangeType.TOPIC, durable=True\n                    )\n\n                    # Declare queue\n                    queue = await channel.declare_queue(queue_name, durable=True)\n\n                    # Bind queue to exchange with routing key\n                    await queue.bind(exchange, routing_key="document.uploaded")\n\n                    print(f"[MARKER WORKER] Waiting for messages in queue '{queue_name}'...")\n\n                    async for message in queue:\n                        if not self.running:\n                            await message.nack(requeue=True)\n                            break\n\n                        async with message.process():\n                            try:\n                                event_with_metadata = json.loads(message.body.decode("utf-8"))\n                                event_payload = event_with_metadata["payload"]\n                                \n                                print(f"[MARKER WORKER] Received message: {event_payload.get('event_type')} for doc {event_payload.get('document_id')}")\n\n                                # Only process DOCUMENT_UPLOADED events\n                                if event_payload.get("event_type") == "DOCUMENT_UPLOADED":\n                                    task = asyncio.create_task(\n                                        self._process_event(event_payload)\n                                    )\n                                    self.processing_tasks.add(task)\n                                    task.add_done_callback(self.processing_tasks.discard)\n\n                                    # Limit concurrent tasks\n                                    if len(self.processing_tasks) >= config.CONCURRENT_WORKERS:\n                                        done, pending = await asyncio.wait(\n                                            self.processing_tasks,\n                                            return_when=asyncio.FIRST_COMPLETED,\n                                        )\n                                        for t in done:\n                                            if t.exception():\n                                                print(f"[MARKER WORKER] Task completed with exception: {t.exception()}")\n                            except Exception as e:\n                                print(f"[MARKER WORKER] Error processing message: {e}")\n            \n            except asyncio.CancelledError:\n                print("[MARKER WORKER] Consumer task cancelled.")\n                break\n            except Exception as e:\n                print(f"[MARKER WORKER] Critical error in consumer: {e}. Retrying in 5 seconds...")\n                await asyncio.sleep(5)\n\n        # Wait for all tasks to complete on shutdown\n        if self.processing_tasks:\n            print(\n                f"[MARKER WORKER] Waiting for {len(self.processing_tasks)} tasks to complete on shutdown..."\n            )\n            await asyncio.wait(self.processing_tasks)\n        \n        print("[MARKER WORKER] Shutdown complete")\n\n\n    async def _process_event(self, event: dict):\n        """Process a single DOCUMENT_UPLOADED event"""\n        doc_id = event["document_id"]\n        s3_path = event["s3_path"]\n        original_filename = event.get("original_filename", f"{doc_id}.pdf")\n\n        print(f"[MARKER WORKER] Processing document: {doc_id}")\n\n        try:\n            await self.database.update_document_status(doc_id, DocumentStatus.PROCESSING)\n\n            with tempfile.TemporaryDirectory(dir=config.TEMP_DIR) as temp_dir:\n                temp_pdf_path = Path(temp_dir) / original_filename\n                \n                # Download PDF from S3\n                await self.storage.download_file(s3_path, str(temp_pdf_path))\n\n                # Run Marker extraction\n                result = await self.executor.extract_document(\n                    input_pdf_path=temp_pdf_path,\n                    output_dir=config.OUTPUT_DIR,\n                    doc_id=doc_id,\n                )\n\n                if result["success"]:\n                    json_s3_path = await self._upload_json_to_s3(\n                        Path(result["json_path"]),\n                        doc_id,\n                    )\n                    \n                    await self.database.update_document_status(doc_id, DocumentStatus.COMPLETED)\n                    \n                    await self.message_broker.publish_event(\n                        {\n                            "event_type": "EXTRACTION_COMPLETED",\n                            "document_id": doc_id,\n                            "s3_json_path": json_s3_path,\n                            "page_count": result["page_count"],\n                            "processing_time_seconds": result["processing_time_seconds"],\n                            "marker_version": result.get("marker_version"),\n                        },\n                        routing_key="document.extracted",\n                    )\n\n                    print(\n                        f"[MARKER WORKER] ✓ Completed: {doc_id} ({result['page_count']} pages)"\n                    )\n\n                else:\n                    error_msg = result.get("error", "Unknown error")\n                    print(f"[MARKER WORKER] ✗ Failed: {doc_id} - {error_msg}")\n                    await self.database.update_document_status(\n                        doc_id,\n                        DocumentStatus.FAILED,\n                        error_msg,\n                    )\n        except Exception as e:\n            print(f"[MARKER WORKER] ✗ Error processing {doc_id}: {e}")\n            await self.database.update_document_status(doc_id, DocumentStatus.FAILED, str(e))\n\n    async def _upload_json_to_s3(self, json_path: Path, doc_id: str) -> str:\n        """Upload Marker JSON to S3"""\n        today = datetime.utcnow()\n        s3_key = f"processed-json/{today.year}/{today.month:02d}/{today.day:02d}/{doc_id}/{json_path.name}"\n        await self.storage.upload_file(str(json_path), s3_key)\n        return f"s3://{config.S3_BUCKET}/{s3_key}"\n\n\n# =========================================================\n# Main Entry Point\n# =========================================================\n\n\nasync def main():\n    """Main entry point for Marker worker"""\n\n    print("[MARKER WORKER] Initializing services...")\n    \n    # Initialize services\n    storage = StorageService(bucket_name=config.S3_BUCKET)\n    database = DatabaseService(connection_string=config.DATABASE_URL)\n    message_broker = MessageBrokerService(broker_url=config.RABBITMQ_URL)\n\n    # Initialize executor\n    executor = MarkerExecutor(use_gpu=config.MARKER_USE_GPU)\n\n    # Create and start worker\n    worker = MarkerWorker(storage, database, message_broker, executor)\n    \n    try:\n        await worker.start()\n    except KeyboardInterrupt:\n        print("[MARKER WORKER] Keyboard interrupt received. Shutting down.")\n    finally:\n        if not worker.running:\n             # If shutdown wasn't triggered by signal handler, do it now\n             worker._signal_handler(signal.SIGINT)\n        # Final wait for tasks to finish\n        if worker.processing_tasks:\n            await asyncio.wait(worker.processing_tasks)\n\n\nif __name__ == "__main__":\n    # To allow graceful shutdown on Windows\n    if os.name == 'nt':\n        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n        \n    asyncio.run(main())